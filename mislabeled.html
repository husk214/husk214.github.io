<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Metric Learning &#8212; papers  documentation</title>
    <link rel="stylesheet" type="text/css" href="static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="static/basic.css" />
    <link rel="stylesheet" href="static/bootswatch-3.3.6/simplex/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="static/bootstrap-sphinx.css" type="text/css" />
    <script data-url_root="./" id="documentation_options" src="static/documentation_options.js"></script>
    <script src="static/jquery.js"></script>
    <script src="static/underscore.js"></script>
    <script src="static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="static/js/jquery-1.11.0.min.js"></script>
    <script src="static/js/jquery-fix.js"></script>
    <script src="static/bootstrap-3.3.6/js/bootstrap.min.js"></script>
    <script src="static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Musgrave ECCV’20 A Metric Learning Reality Check" href="metriclearning/reality_check.html" />
    <link rel="prev" title="Northcutt ICML’20 Confident Learning: Estimating Uncertainty in Dataset Labels" href="mislabeled/confident_learning.html" />
<link rel="stylesheet" href="static/custom.css" type="text/css" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74773673-1', 'auto');
  ga('send', 'pageview');
</script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          ashibaga</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="work.html">Work</a><ul>
<li class="toctree-l2"><a class="reference internal" href="work/hotel_spot_search.html">ホテルスポット検索</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metriclearning.html">Mislabeled</a><ul>
<li class="toctree-l2"><a class="reference internal" href="metriclearning.html#papers">Papers</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Metric Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#papers">Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Metric Learningとは</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="other.html">Other</a><ul>
<li class="toctree-l2"><a class="reference internal" href="other.html#papers">Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="other.html#memo">Memo</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ltr.html">Learning to Rank</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ltr.html#papers">Papers</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr.html#id1">Learning to Rank (ランク学習)とは</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Metric Learning</a><ul>
<li><a class="reference internal" href="#papers">Papers</a></li>
<li><a class="reference internal" href="#id1">Metric Learningとは</a><ul>
<li><a class="reference internal" href="#id2">応用先</a></li>
<li><a class="reference internal" href="#id3">ロス関数</a><ul>
<li><a class="reference internal" href="#embedding-losses">Embedding losses</a></li>
<li><a class="reference internal" href="#classification-losses">Classification losses</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id18">参考文献</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="mislabeled/confident_learning.html" title="Previous Chapter: Northcutt ICML’20 Confident Learning: Estimating Uncertainty in Dataset Labels"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Northcutt ICM...</span>
    </a>
  </li>
  <li>
    <a href="metriclearning/reality_check.html" title="Next Chapter: Musgrave ECCV’20 A Metric Learning Reality Check"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Musgrave ECCV... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul>
<li><a class="reference internal" href="#">Metric Learning</a><ul>
<li><a class="reference internal" href="#papers">Papers</a></li>
<li><a class="reference internal" href="#id1">Metric Learningとは</a><ul>
<li><a class="reference internal" href="#id2">応用先</a></li>
<li><a class="reference internal" href="#id3">ロス関数</a><ul>
<li><a class="reference internal" href="#embedding-losses">Embedding losses</a></li>
<li><a class="reference internal" href="#classification-losses">Classification losses</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id18">参考文献</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <li>
    <a href="mislabeled/confident_learning.html" title="Previous Chapter: Northcutt ICML’20 Confident Learning: Estimating Uncertainty in Dataset Labels"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm hidden-tablet">&laquo; Northcutt ICM...</span>
    </a>
  </li>
  <li>
    <a href="metriclearning/reality_check.html" title="Next Chapter: Musgrave ECCV’20 A Metric Learning Reality Check"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Musgrave ECCV... &raquo;</span>
    </a>
  </li>
<div id="sourcelink">
  <a href="sources/mislabeled.rst.txt"
     rel="nofollow">Source</a>
</div>
<form action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="col-md-9 content">
      
  <section id="metric-learning">
<h1>Metric Learning<a class="headerlink" href="#metric-learning" title="Permalink to this headline">¶</a></h1>
<section id="papers">
<h2>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="metriclearning/reality_check.html">Musgrave ECCV’20 A Metric Learning Reality Check</a></li>
<li class="toctree-l1"><a class="reference internal" href="metriclearning/lifted_structured.html">Song CVPR’16 Deep Metric Learning via Lifted Structured Feature Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="metriclearning/multi_similarity.html">Wang CVPR19 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="metriclearning/multi_similarity.html#wang-cvpr-20-cross-batch-memory-for-embedding-learning">Wang CVPR’20 Cross-Batch Memory for Embedding Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="metriclearning/proxy_nca.html">Movshovitz ICCV’17 No fuss distance metric learning using proxies</a></li>
<li class="toctree-l1"><a class="reference internal" href="metriclearning/proxy_anchor.html">Kim CVPR’20 Proxy Anchor Loss for Deep Metric Learning</a></li>
</ul>
</div>
</section>
<section id="id1">
<h2>Metric Learningとは<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>一般的に記述できる枠組みはよくわからない</p>
<ul>
<li><p>ふんわりしすぎている印象 (なんでもMetric Learningと言ってしまえるような感じがする)</p></li>
<li><p>応用先によって問題設定が異なるので何とも言えないと思っている</p></li>
</ul>
</li>
<li><p>雰囲気は、いい感じにデータを変換する関数を学習することをMetric Learningと呼んでいる気がする</p></li>
</ul>
<section id="id2">
<h3>応用先<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>顔認証</p>
<ul>
<li><p>同一人物の画像どうしの距離を小さく、違う人物どうしの距離を遠くするように関数を学習する</p></li>
</ul>
</li>
<li><p>情報検索(例えば画像検索)</p>
<ul>
<li><p>同じような画像どうしの距離を小さく、違う感じの画像どうしの距離を遠くするように関数を学習する</p></li>
</ul>
</li>
<li><p>異常検知</p></li>
<li><p>Learning to Rank</p></li>
<li><p>etc…</p></li>
</ul>
</section>
<section id="id3">
<h3>ロス関数<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>分類問題のロス(Cross Entropy)から派生したロス(Classification losses)とそうでないもの(Embedding losses)がある by <a class="reference internal" href="#musgrave20" id="id4"><span>[Musgrave20]</span></a></p></li>
<li><p><a class="reference internal" href="#musgrave20" id="id5"><span>[Musgrave20]</span></a> によると、Proxy族は Classification losses になっているが、Embedding lossesだと思う</p></li>
</ul>
<p><strong>Notation</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S_{ij} := s(x_{i}, x_{j})\)</span>, サンプルiとサンプルjの類似度</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x_{i}\)</span> : サンプルiのembedding</p></li>
<li><p><span class="math notranslate nohighlight">\(s(\cdot, \cdot)\)</span> : 2つのサンプルの類似度を測る関数 (例: 内積, cosine類似度)</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\([x]_+ := \max(x, 0)\)</span></p></li>
</ul>
<section id="embedding-losses">
<h4>Embedding losses<a class="headerlink" href="#embedding-losses" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><strong>Contrastive loss</strong> <a class="reference internal" href="#hadsel06" id="id6"><span>[Hadsel06]</span></a> - 同じラベルなら類似度が高く、違うラベルなら低くなるように学習する</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(I_{ij} := 1 ~~\text{if}~~ y_i = y_j ~~\text{else}~~ 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> : サンプルiのラベル</p></li>
<li><p><span class="math notranslate nohighlight">\(m_p, m_n\)</span> : positive pairのマージン(例: 1.0), negative pairのマージン (例: 0.5)</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\begin{align}
  L_{constract}(x_i, x_j) := I_{ij} [m_p - s(x_i, x_j) ]_+ + (1-I_{ij}) [s(x_i, x_j) - m_n]_+
\end{align}</div><ul class="simple">
<li><p><strong>Triplet loss</strong> <a class="reference internal" href="#hoffer15" id="id7"><span>[Hoffer15]</span></a> - aとpの類似度より、aとnの類似度が低くなるように学習する</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(a, n, p\)</span> : a=anchorサンプル, n=aのnegative pair, p=aのpositive pair</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\begin{align}
  L_{triplet}(x_a, x_p, x_n) := [s(x_a, x_n) - s(x_a, x_p) + m ]_+
\end{align}</div><ul class="simple">
<li><p><strong>N-pair loss</strong> <a class="reference internal" href="#sohn16" id="id8"><span>[Sohn16]</span></a> - N-pairに拡張したもの</p></li>
</ul>
<div class="math notranslate nohighlight">
\begin{align}
  L_{N-pair-mc} (\{x_i, x^+_i\}_{i=1}^N) &amp;:= \cfrac{1}{N} \sum_{i=1}^{N}
    \log \left(1 + \sum_{j \neq i } \exp(s(x_i, x_j^+) - s(x_i, x^+_i) ) \right) \\
  L_{N-pair-ovo} (\{x_i, x^+_i\}_{i=1}^N) &amp;:= \cfrac{1}{N} \sum_{i=1}^{N}
    \sum_{j \neq i } \log \left(1 + \exp (s(x_i, x_j^+) - s(x_i, x^+_i) ) \right)
\end{align}</div><ul class="simple">
<li><p><strong>Lifted Structure loss</strong> <a class="reference internal" href="#song16" id="id9"><span>[Song16]</span></a> <a class="reference internal" href="metriclearning/lifted_structured.html"><span class="doc">Song CVPR’16 Deep Metric Learning via Lifted Structured Feature Embedding</span></a></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span> : ミニバッチ (embeddingの集合)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}^+_{x_i}\)</span> : ミニバッチ内の <span class="math notranslate nohighlight">\(x_i\)</span> と同じクラスのembeddingの集合</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}^-_p\)</span> : ミニバッチ内の <span class="math notranslate nohighlight">\(x_i\)</span> とは違うクラスのembeddingの集合</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\begin{align}
  L_{lifted} (\mathcal{X}) &amp;:= \sum_{x_i \in \mathcal{X}}
    \left( \log \sum_{x_k \in \mathcal{X}_{x_i}^+} \exp(m - s(x_i, x_k)) + \log \sum_{x_k \in \mathcal{X}_{x_i}^-} \exp( s(x_i, x_k)) \right)
\end{align}</div><ul class="simple">
<li><p><strong>Multi Similarity loss</strong> <a class="reference internal" href="#wang19" id="id10"><span>[Wang19]</span></a> <a class="reference internal" href="metriclearning/multi_similarity.html"><span class="doc">Wang CVPR19 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</span></a></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\alpha, \beta\)</span> : スケーリングパラメータ(ハイパーパラメータ)</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\begin{align}
  L_{MS} (\mathcal{X}) &amp;:= \cfrac{1}{|\mathcal{X}|} \left\{
  \cfrac{1}{\alpha} \log \left(1+ \sum_{x_k \in \mathcal{X}_{x_i}^+} \exp(-\alpha (s(x_i, x_k) - m)) \right)
  + \cfrac{1}{\beta} \log \left(1+ \sum_{x_k \in \mathcal{X}_{x_i}^-} \exp(\beta (s(x_i, x_k) - m)) \right) \right\}
\end{align}</div><p><strong>Proxy族: 各クラスを代表するProxiesを使ったロス</strong></p>
<ul class="simple">
<li><p><strong>Proxy NCA</strong> <a class="reference internal" href="#movshovitz17" id="id11"><span>[Movshovitz17]</span></a> <a class="reference internal" href="metriclearning/proxy_nca.html"><span class="doc">Movshovitz ICCV’17 No fuss distance metric learning using proxies</span></a></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{P} = \{p_1, \ldots, p_L \}\)</span>, proxies (trainableな変数として、backpropの対象にして学習していく, L=クラス数)</p></li>
<li><p><span class="math notranslate nohighlight">\(d\)</span> : 距離関数 (例: L2距離)</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\begin{align}
  L_{NCA}(x) := -
  \log \cfrac{\exp(-d(x, p_y ))}
  {\sum_{p \in \mathcal{P} \setminus \{p_y\} } \exp( -d(x, p))}
\end{align}</div><ul class="simple">
<li><p><strong>SoftTriplet</strong> <a class="reference internal" href="#qian19" id="id12"><span>[Qian19]</span></a></p>
<ul>
<li><p>ProxyNCA like</p></li>
<li><p>proxyを各クラスK個用意して、relaxed similarityを導入している</p></li>
<li><p>softmax lossがsmoothedなtriplet lossであるとか言っていて興味深いが、詳しく読めていない</p></li>
</ul>
</li>
<li><p><strong>Proxy Anchor</strong> <a class="reference internal" href="#kim20" id="id13"><span>[Kim20]</span></a> <a class="reference internal" href="metriclearning/proxy_anchor.html"><span class="doc">Kim CVPR’20 Proxy Anchor Loss for Deep Metric Learning</span></a></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{P}^+, \mathcal{P}^-\)</span> : ミニバッチ内のデータのクラスのproxyの集合, ミニバッチ内のデータには存在しないクラスのproxyの集合</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}^+_p, \mathcal{X}^-_p\)</span> : ミニバッチ内のproxy pと同じクラスのembeddingの集合, ミニバッチ内のproxy pとは違うクラスのembeddingの集合</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> : スケーリングパラメータ (ハイパーパラメータ)</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\begin{align}
  L_{PA}(\mathcal{X}) := \cfrac{1}{|\mathcal{P}^+|}
  \sum_{p \in \mathcal{P}^+}
  \log \left(1 + \sum_{x \in \mathcal{X}^+_p} \exp(-\alpha (s(p, x) - m )) \right)
  + \frac{1}{|\mathcal{P}^-|}
  \sum_{p \in \mathcal{P}^-}
  \log \left(1 + \sum_{x \in \mathcal{X}^-_p} \exp(\alpha (s(p, x) + m )) \right)
\end{align}</div></section>
<section id="classification-losses">
<h4>Classification losses<a class="headerlink" href="#classification-losses" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Center loss <a class="reference internal" href="#wen16" id="id14"><span>[Wen16]</span></a></p></li>
<li><p>SphereFace <a class="reference internal" href="#liu17" id="id15"><span>[Liu17]</span></a></p></li>
<li><p>CosFace <a class="reference internal" href="#wang18" id="id16"><span>[Wang18]</span></a></p></li>
<li><p>ArcFace <a class="reference internal" href="#deng19" id="id17"><span>[Deng19]</span></a></p></li>
</ul>
</section>
</section>
<section id="id18">
<h3>参考文献<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<dl class="citation">
<dt class="label" id="hadsel06"><span class="brackets"><a class="fn-backref" href="#id6">Hadsel06</a></span></dt>
<dd><ol class="upperalpha simple" start="18">
<li><p>Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006.</p></li>
</ol>
</dd>
<dt class="label" id="hoffer15"><span class="brackets"><a class="fn-backref" href="#id7">Hoffer15</a></span></dt>
<dd><ol class="upperalpha simple" start="5">
<li><p>Hoffer and N. Ailon. Deep metric learning using triplet network. In SIMBAD, 2015.</p></li>
</ol>
</dd>
<dt class="label" id="sohn16"><span class="brackets"><a class="fn-backref" href="#id8">Sohn16</a></span></dt>
<dd><ol class="upperalpha simple" start="11">
<li><p>Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NIPS 2016.</p></li>
</ol>
</dd>
<dt class="label" id="song16"><span class="brackets"><a class="fn-backref" href="#id9">Song16</a></span></dt>
<dd><ol class="upperalpha simple" start="8">
<li><p>Oh Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep metric learning via lifted structured feature embedding. In CVPR, 2016.</p></li>
</ol>
</dd>
<dt class="label" id="wang19"><span class="brackets"><a class="fn-backref" href="#id10">Wang19</a></span></dt>
<dd><ol class="upperalpha simple" start="24">
<li><p>Wang, X. Han, W. Huang, D. Dong, and M.R. Scott. Multi-similarity loss with general pair weighting for deep metric learning. In CVPR 2019.</p></li>
</ol>
</dd>
<dt class="label" id="movshovitz17"><span class="brackets"><a class="fn-backref" href="#id11">Movshovitz17</a></span></dt>
<dd><ol class="upperalpha simple" start="25">
<li><p>Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, and S. Singh. No fuss distance metric learning using proxies. In ICCV 2017.</p></li>
</ol>
</dd>
<dt class="label" id="qian19"><span class="brackets"><a class="fn-backref" href="#id12">Qian19</a></span></dt>
<dd><p>Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, Rong Jin. SoftTriple Loss: Deep Metric Learning Without Triplet Sampling. In ICCV, 2019.</p>
</dd>
<dt class="label" id="kim20"><span class="brackets"><a class="fn-backref" href="#id13">Kim20</a></span></dt>
<dd><ol class="upperalpha simple" start="19">
<li><p>Kim, D. Kim, M. Cho, and S. Kwak. Proxy Anchor Loss for Deep Metric Learning. In CVPR 2020.</p></li>
</ol>
</dd>
<dt class="label" id="wen16"><span class="brackets"><a class="fn-backref" href="#id14">Wen16</a></span></dt>
<dd><p>Yandong Wen, Kaipeng Zhang, Zhifeng Li, Yu Qiao. A Discriminative Feature Learning Approach for Deep Face Recognition. In ECCV 2016.</p>
</dd>
<dt class="label" id="liu17"><span class="brackets"><a class="fn-backref" href="#id15">Liu17</a></span></dt>
<dd><p>Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song. SphereFace: Deep Hypersphere Embedding for Face Recognition. In CVPR 2017.</p>
</dd>
<dt class="label" id="wang18"><span class="brackets"><a class="fn-backref" href="#id16">Wang18</a></span></dt>
<dd><p>Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, Wei Liu. CosFace: Large Margin Cosine Loss for Deep Face Recognition. In CVPR 2018.</p>
</dd>
<dt class="label" id="deng19"><span class="brackets"><a class="fn-backref" href="#id17">Deng19</a></span></dt>
<dd><p>Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. In CVPR 2019.</p>
</dd>
<dt class="label" id="musgrave20"><span class="brackets">Musgrave20</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Kevin Musgrave, Serge Belongie, Ser-Nam Lim. A Metric Learning Reality Check. In ECCV 2020.</p>
</dd>
</dl>
</section>
</section>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2021, ashibaga.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.3.2.<br/>
    </p>
  </div>
</footer>
  </body>
</html>