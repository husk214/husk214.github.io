<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Atsushi Shibagaki (柴垣篤志) &#8212; papers  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=db26dd79" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=579adecb" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=eab45d89" />
    <link rel="stylesheet" href="_static/bootswatch-3.3.6/simplex/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=13a9ecda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <script src="_static/js/jquery-1.11.0.min.js"></script>
    <script src="_static/js/jquery-fix.js"></script>
    <script src="_static/bootstrap-3.3.6/js/bootstrap.min.js"></script>
    <script src="_static/bootstrap-sphinx.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Metric Learning" href="metriclearning.html" />
<link rel="stylesheet" href="_static/custom.css" type="text/css" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74773673-1', 'auto');
  ga('send', 'pageview');
</script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="#">
          ashibaga</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul>
<li class="toctree-l1"><a class="reference internal" href="metriclearning.html">Metric Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/reality_check.html">Musgrave ECCV’20 A Metric Learning Reality Check</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/lifted_structured.html">Song CVPR’16 Deep Metric Learning via Lifted Structured Feature Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/multi_similarity.html">Wang CVPR19 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/multi_similarity.html#wang-cvpr-20-cross-batch-memory-for-embedding-learning">Wang CVPR’20 Cross-Batch Memory for Embedding Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/proxy_nca.html">Movshovitz ICCV’17 No fuss distance metric learning using proxies</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/proxy_anchor.html">Kim CVPR’20 Proxy Anchor Loss for Deep Metric Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/s2sd.html">Roth ICML’21 Simultaneous Similarity-based Self-Distillation for Deep Metric Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/clip.html">Radford ICML’21 CLIP (Learning Transferable Visual Models From Natural Language Supervision)</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/lang_guide.html">Roth CVPR’22 Integrating Language Guidance into Vision-based Deep Metric Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="noisylabel.html">Learning from Noisy Labels</a><ul>
<li class="toctree-l2"><a class="reference internal" href="noisylabel/confident_learning.html">Northcutt ICML’20 Confident Learning: Estimating Uncertainty in Dataset Labels</a></li>
<li class="toctree-l2"><a class="reference internal" href="noisylabel/pervasive_label_errors.html">Northcutt NeurIPS’21 Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ssl.html">Self Supervised Learning (SSL)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ssl/simclr.html">Chen ICML’20 SimCLR (A Simple Framework for Contrastive Learning of Visual Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ssl/byol.html">Grill NIPS’20 BYOL (Bootstrap Your Own Latent A New Approach to Self-Supervised Learning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ssl/simsiam.html">Chen CVPR’21 SimSiam (Exploring Simple Siamese Representation Learning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ssl/how_avoid.html">Does ICLR’22 How Does SimSiam Avoid Collapse Without Negative Samples?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="representation.html">Representation Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="representation/understaing_crl.html">Wang ICML’20 Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="other.html">Other</a><ul>
<li class="toctree-l2"><a class="reference internal" href="other/sentencepiece.html">Kudo EMNLP’18 SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/optimizer_benchmark.html">Teja ICML’20 Optimizer Benchmarking Needs to Account for Hyperparameter Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/user_centric_ranking.html">Zhao (KDD’23) Breaking the Curse of Quality Saturation with User-Centric Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/quert.html">Xie (KDD’23) QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/tensorflow_serving.html">Tensorflow Serving</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/search_engine_qu.html">検索システム 実務者のための開発改善ガイドブック 11章 検索を成功させるための支援</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="proceedings.html">Proceedings</a><ul>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir23_abst.html">SIGIR’23 ABSTRACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir22_abst.html">SIGIR’22 ABSTRACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir21_abst.html">SIGIR’21 ABSTRACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir20_abst.html">SIGIR’20 ABSTRACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir19_abst.html">SIGIR’19 ABSTRACT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ltr.html">Learning to Rank</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ltr/dasalc.html">Zhen ICLR’21 Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/mixture_transformation.html">Zhuang SIGIR’20 Feature Transformation for Neural Ranking Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/neuralsort.html">Grover ICLR’19 Stochastic Optimization of Sorting Networks via Continuous Relaxations</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/otsort.html">Cuturi NeurIPS’19 Differentiable Ranks and Sorting using Optimal Transport</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/fastsort.html">Blondel ICML’20 Fast Differentiable Sorting and Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/diffsortnet.html">Petersen ICML’21 Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/monodiffsort.html">Petersen ICLR’22 Monotonic Differentiable Sorting Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/algorithm.html">Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/metric.html">Metric</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="agri.html">畑</a><ul>
<li class="toctree-l2"><a class="reference internal" href="agri/diary.html">Diary</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Atsushi Shibagaki (柴垣篤志)</a><ul>
<li><a class="reference internal" href="#preprint">Preprint</a></li>
<li><a class="reference internal" href="#publications">Publications</a></li>
<li><a class="reference internal" href="#activities">Activities</a></li>
</ul>
</li>
<li><a class="reference internal" href="#memo">Memo</a></li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="metriclearning.html" title="Next Chapter: Metric Learning"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Metric Learning &raquo;</span>
    </a>
  </li>
              
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="col-md-3">
        <div id="sidebar" class="bs-sidenav" role="complementary"><ul>
<li><a class="reference internal" href="#">Atsushi Shibagaki (柴垣篤志)</a><ul>
<li><a class="reference internal" href="#preprint">Preprint</a></li>
<li><a class="reference internal" href="#publications">Publications</a></li>
<li><a class="reference internal" href="#activities">Activities</a></li>
</ul>
</li>
<li><a class="reference internal" href="#memo">Memo</a></li>
</ul>

  <li>
    <a href="metriclearning.html" title="Next Chapter: Metric Learning"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm hidden-tablet">Metric Learning &raquo;</span>
    </a>
  </li>
<div id="sourcelink">
  <a href="_sources/index.rst.txt"
     rel="nofollow">Source</a>
</div>
<form action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="col-md-9 content">
      
  <figure class="top_float">
  <img src="_static/logo.jpg" class="img-thumbnail">
</figure><section id="atsushi-shibagaki">
<h1>Atsushi Shibagaki (柴垣篤志)<a class="headerlink" href="#atsushi-shibagaki" title="Link to this heading">¶</a></h1>
<p>I found employment.</p>
<div class="contact">
<span class="octicon octicon-mail"></span> <font color='#7ca428'>shibagaki.a.mllab.nit _at_ gmail.com </font> <br>
<span class="octicon octicon-mark-github"></span> <a href="https://github.com/husk214">GitHub</a>
</div><section id="preprint">
<h2>Preprint<a class="headerlink" href="#preprint" title="Link to this heading">¶</a></h2>
<div class="menu">
    <label for="Panelp2">
    <strong>Stochastic Primal Dual Coordinate Method with Non-Uniform Sampling Based on Optimality Violations</strong> <br>
    <u><i>A. Shibagaki</i> and I. Takeuchi</u> <br>
    <div class="mybtn3"><a href="https://arxiv.org/abs/1703.07056">arXiv:1703.07056</a></div>
    </label>
    <input type="checkbox" id="Panelp2" class="on-off"/>
    <ul> <div class="abst"> <b>Abstract: </b>
    We study primal-dual type stochastic optimization algorithms with non-uniform sampling. Our main theoretical contribution in this paper is to present a convergence analysis of Stochastic Primal Dual Coordinate (SPDC) Method with arbitrary sampling. Based on this theoretical framework, we propose Optimality Violation-based Sampling SPDC (ovsSPDC), a non-uniform sampling method based on Optimality Violation. We also propose two efficient heuristic variants of ovsSPDC called ovsSDPC+ and ovsSDPC++. Through intensive numerical experiments, we demonstrate that the proposed method and its variants are faster than other state-of-the-art primal-dual type stochastic optimization methods. </div> </ul> <br>
</div></section>
<section id="publications">
<h2>Publications<a class="headerlink" href="#publications" title="Link to this heading">¶</a></h2>
<p><span class="pubs">International Conferences (refereed)</span></p>
<div class="menu">
    <label for="Panel3">
    <strong>Efficiently Monitoring Small Data Modification Effect for Large-Scale Learning in Changing Environment</strong> <br>
    <u>H. Hanada, <i>A. Shibagaki</i>, J. Sakuma, and I. Takeuchi</u>  <br>
    Proceedings of The 32nd AAAI Conference on Artificial Intelligence (<a href="https://aaai.org/Conferences/AAAI-18/">AAAI2018</a>), pp. 1314-1321, New Orleans, Louisiana, USA, February 2018 <i>(acceptance rate =  933/3800 = 24.6%)</i> <br>
    <div class="mybtn3"><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16089">paper</a></div>
    <div class="mybtn3"><a href="http://arxiv.org/abs/1606.00136">arXiv</a></div> <br>
    </label>
    <input type="checkbox" id="Panel3" class="on-off"/>
    <ul> <div class="abst"> <b>Abstract: </b>
    We study large-scale machine learning problems in changing environments where a small part of the dataset is modified, and the effect of the data modification must be monitored in order to know how much the modification changes the optimal model. When the entire dataset is large, even if the amount of the data modification is fairly small, the computational cost for re-training the model would be prohibitively large. In this paper, we propose a novel method, called the optimal solution bounding (OSB), for monitoring such a data modification effect on the optimal model by efficiently evaluating (without actually re-training) it. The proposed method provides bounds on the unknown optimal model with the cost proportional only to the size of the data modification. </div> </ul> <br>
    <label for="Panel1">
    <strong>Simultaneous Safe Screening of Features and Samples in Doubly Sparse Modeling</strong> <br>
    <u><i>A. Shibagaki</i>, M. Karasuyama, K. Hatano and I. Takeuchi</u> <br>
    Proceedings of The 33rd International Conference on Machine Learning (<a href="http://icml.cc/2016/">ICML2016</a>), pp. 1577–1586, New York, NYC, USA, June 2016 <i>(acceptance rate = 322/1327 = 24.3%)</i> <br>
    <div class="mybtn3"><a href="http://jmlr.org/proceedings/papers/v48/shibagaki16.html">paper</a></div>
    <div class="mybtn3"><a href="http://arxiv.org/abs/1602.02485">arXiv</a></div>
    <div class="mybtn3"><a href="https://github.com/husk214/s3fs">code</a></div> <br>
    </label>
    <input type="checkbox" id="Panel1" class="on-off"/>
    <ul> <div class="abst"> <b>Abstract: </b>
    The problem of learning a sparse model is conceptually interpreted as the process of identifying active features/samples and then optimizing the model over them. Recently introduced safe screening allows us to identify a part of nonactive features/samples. So far, safe screening has been individually studied either for feature screening or for sample screening. In this paper, we introduce a new approach for safely screening features and samples simultaneously by alternatively iterating feature and sample screening steps. A significant advantage of considering them simultaneously rather than individually is that they have a synergy effect in the sense that the results of the previous safe feature screening can be exploited for improving the next safe sample screening performances, and viceversa. We first theoretically investigate the synergy effect, and then illustrate the practical advantage through intensive numerical experiments for problems with large numbers of features and samples. </div> </ul> <br>
    <label for="Panel2">
    <strong>Regularization Path of Cross-Validation Error Lower Bounds</strong> <br>
    <u><i>A. Shibagaki</i>, Y. Suzuki, M. Karasuyama and I. Takeuchi</u> <br>
     Proceedings of the 29th Neural Information Processing Systems (<a href="https://nips.cc/Conferences/2015">NIPS2015</a>), pp. 1666-1674, Montreal, Quebec, Canada, December 2015 <i>(acceptance rate = 403/1838 = 21.9 % )</i> <br>
    <div class="mybtn3"><a href="http://papers.nips.cc/paper/5817-regularization-path-of-cross-validation-error-lower-bounds">paper</a></div>
    <div class="mybtn3"><a href="http://arxiv.org/abs/1602.02485">arXiv</a></div>
    <div class="mybtn3"><a href="misc/shibagaki_poster_nips15.pdf">poster</a></div> <br>
    </label>
    <input type="checkbox" id="Panel2" class="on-off"/>
    <ul> <div class="abst"> <b>Abstract: </b>
    Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances. Nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error. In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter, which we call regularization path of CV error lower bounds. The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters. Our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs. </div> </ul> <br>
</div><p><span class="pubs">Domestic Conferences (not refereed)</span></p>
<div class="papercard">
    <strong>変数の保持と削除に関するセーフルールによるスパースモデル最適化問題のサイズ縮小と高速化に関する一考察</strong> <br>
    <u>烏山昌幸, <i>柴垣篤志</i>, 竹内一郎</u> <br>
    信学技報, vol. 116, no. 500, <a href="http://ibisml.org/ibisml028">IBISML</a> 2016-107, pp. 57-62, 東京, 2017年3月. <br>
    <div class="mybtn3"><a href="http://www.ieice.org/ken/paper/201703076bSz/">paper</a></div>
</div>

<div class="papercard">
    <strong>経験損失最小化問題における高速感度分析に関する一提案</strong> <br>
    <u>花田博幸, <i>柴垣篤志</i>, 佐久間淳, 竹内一郎</u> <br>
    信学技報, vol. 116, no. 209, <a href="http://ibisml.org/ibisml026">IBISML</a> 2016-35, pp. 203-210, 富山, 2016年9月. <br>
    <div class="mybtn3"><a href="http://www.ieice.org/ken/paper/20160906Xbkj/">paper</a></div>
</div>

<div class="papercard">
    <strong>スパースモデルのための特徴と標本の同時セーフスクリーニング</strong> <br>
    <u><i>柴垣篤志</i>, 烏山昌幸, 畑埜晃平, 竹内一郎</u> <br>
    信学技報, vol. 116, no. 121, <a href="http://ibisml.org/ibisml025">IBISML</a> 2016-4, pp. 201-208, 沖縄, 2016年7月. <br>
    <div class="mybtn3"><a href="http://www.ieice.org/ken/paper/20160706YbjI/">paper</a></div>
</div>

<div class="papercard">
    <strong>L2正則化凸損失関数最小化問題のための検証誤差近似保証付きモデル選択</strong><br>
    <u><i>柴垣篤志</i>, 鈴木良規, 竹内一郎</u> <br>
    信学技報, vol. 114, no. 502, <a href="http://ibisml.org/ibisml020">IBISML</a> 2014-96, pp. 79-86, 京都, 2015年3月.<br>
    <div class="mybtn3"><a href="http://www.ieice.org/ken/paper/20150306DBZ8/">paper</a></div>
</div></section>
<section id="activities">
<h2>Activities<a class="headerlink" href="#activities" title="Link to this heading">¶</a></h2>
<ul class="actli simple">
<li><p><strong>Mar (2017):</strong>
Got master’s degree form Nagoya Institute of Technology (Department of Scientific and Engineering Simulation)</p></li>
<li><p><strong>Dec (2016):</strong>
Poster presentation in <a class="reference external" href="http://bigdata.nii.ac.jp/johokei-winterfesta2/program">情報系 WINTER FESTA Episode2</a> &#64; Tokyo</p></li>
<li><p><strong>Nov (2016):</strong>
Poster presentation in <a class="reference external" href="http://ibisml.org/ibis2016/poster1/">IBIS2016</a> &#64; Kyoto Univ.</p></li>
<li><p><strong>Jul (2016):</strong>
Oral presentation in <a class="reference external" href="http://ibisml.org/ibisml025">IBISML25</a> &#64; Okinawa.</p></li>
<li><p><strong>Jun (2016):</strong>
Poster presentation in <a class="reference external" href="http://icml.cc/2016/">ICML 2016</a> &#64; New York City</p></li>
<li><p><strong>Dec (2015):</strong>
Poster presentation in <a class="reference external" href="https://nips.cc/Conferences/2015">NIPS 2015</a> &#64; Montreal</p></li>
<li><p><strong>Nov (2015):</strong>
Poster presentation in <a class="reference external" href="http://ibisml.org/ibis2015/poster1/">IBIS2015</a> &#64; Tsukuba</p></li>
<li><p><strong>Aug - Sep (2015):</strong>
Poster presentation in <a class="reference external" href="http://www.iip.ist.i.kyoto-u.ac.jp/mlss15/doku.php/">Machine Learning Summer School (MLSS) 2015</a> &#64; Kyoto Univ.</p></li>
<li><p><strong>Mar (2015):</strong>
Got bachelor’s degree form Nagoya Institute of Technology (Department of Computer Science)
<strong>Award:</strong> <a class="reference external" href="http://www.denei.jp/news/1427331600.html">電影会賞</a></p></li>
<li><p><strong>Mar (2015):</strong>
Oral presentation in <a class="reference external" href="http://ibisml.org/ibisml020">IBISML20</a> &#64; Kyoto Univ.</p></li>
</ul>
</section>
</section>
<section id="memo">
<h1>Memo<a class="headerlink" href="#memo" title="Link to this heading">¶</a></h1>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="metriclearning.html">Metric Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/reality_check.html">Musgrave ECCV’20 A Metric Learning Reality Check</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/lifted_structured.html">Song CVPR’16 Deep Metric Learning via Lifted Structured Feature Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/multi_similarity.html">Wang CVPR19 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/multi_similarity.html#wang-cvpr-20-cross-batch-memory-for-embedding-learning">Wang CVPR’20 Cross-Batch Memory for Embedding Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/proxy_nca.html">Movshovitz ICCV’17 No fuss distance metric learning using proxies</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/proxy_anchor.html">Kim CVPR’20 Proxy Anchor Loss for Deep Metric Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/s2sd.html">Roth ICML’21 Simultaneous Similarity-based Self-Distillation for Deep Metric Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/clip.html">Radford ICML’21 CLIP (Learning Transferable Visual Models From Natural Language Supervision)</a></li>
<li class="toctree-l2"><a class="reference internal" href="metriclearning/lang_guide.html">Roth CVPR’22 Integrating Language Guidance into Vision-based Deep Metric Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="noisylabel.html">Learning from Noisy Labels</a><ul>
<li class="toctree-l2"><a class="reference internal" href="noisylabel/confident_learning.html">Northcutt ICML’20 Confident Learning: Estimating Uncertainty in Dataset Labels</a></li>
<li class="toctree-l2"><a class="reference internal" href="noisylabel/pervasive_label_errors.html">Northcutt NeurIPS’21 Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ssl.html">Self Supervised Learning (SSL)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ssl/simclr.html">Chen ICML’20 SimCLR (A Simple Framework for Contrastive Learning of Visual Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ssl/byol.html">Grill NIPS’20 BYOL (Bootstrap Your Own Latent A New Approach to Self-Supervised Learning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ssl/simsiam.html">Chen CVPR’21 SimSiam (Exploring Simple Siamese Representation Learning)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ssl/how_avoid.html">Does ICLR’22 How Does SimSiam Avoid Collapse Without Negative Samples?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="representation.html">Representation Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="representation/understaing_crl.html">Wang ICML’20 Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="other.html">Other</a><ul>
<li class="toctree-l2"><a class="reference internal" href="other/sentencepiece.html">Kudo EMNLP’18 SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/optimizer_benchmark.html">Teja ICML’20 Optimizer Benchmarking Needs to Account for Hyperparameter Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/user_centric_ranking.html">Zhao (KDD’23) Breaking the Curse of Quality Saturation with User-Centric Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/quert.html">Xie (KDD’23) QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/tensorflow_serving.html">Tensorflow Serving</a></li>
<li class="toctree-l2"><a class="reference internal" href="other/search_engine_qu.html">検索システム 実務者のための開発改善ガイドブック 11章 検索を成功させるための支援</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="proceedings.html">Proceedings</a><ul>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir23_abst.html">SIGIR’23 ABSTRACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir22_abst.html">SIGIR’22 ABSTRACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir21_abst.html">SIGIR’21 ABSTRACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir20_abst.html">SIGIR’20 ABSTRACT</a></li>
<li class="toctree-l2"><a class="reference internal" href="proceedings/sigir19_abst.html">SIGIR’19 ABSTRACT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ltr.html">Learning to Rank</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ltr/dasalc.html">Zhen ICLR’21 Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/mixture_transformation.html">Zhuang SIGIR’20 Feature Transformation for Neural Ranking Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/neuralsort.html">Grover ICLR’19 Stochastic Optimization of Sorting Networks via Continuous Relaxations</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/otsort.html">Cuturi NeurIPS’19 Differentiable Ranks and Sorting using Optimal Transport</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/fastsort.html">Blondel ICML’20 Fast Differentiable Sorting and Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/diffsortnet.html">Petersen ICML’21 Differentiable Sorting Networks for Scalable Sorting and Ranking Supervision</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/monodiffsort.html">Petersen ICLR’22 Monotonic Differentiable Sorting Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/algorithm.html">Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="ltr/metric.html">Metric</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="agri.html">畑</a><ul>
<li class="toctree-l2"><a class="reference internal" href="agri/diary.html">Diary</a></li>
</ul>
</li>
</ul>
</div>
</section>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 8.2.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>